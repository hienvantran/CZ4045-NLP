{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from transformers import InputExample, InputFeatures\n",
    "from transformers import TFDistilBertForSequenceClassification\n",
    "\n",
    "\n",
    "model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  109482240 \n",
      "                                                                 \n",
      " dropout_37 (Dropout)        multiple                  0         \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  1538      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 109,483,778\n",
      "Trainable params: 109,483,778\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data_to_examples(train, test, DATA_COLUMN, LABEL_COLUMN):\n",
    "    train_InputExamples = train.apply(\n",
    "        lambda x: InputExample(\n",
    "            guid=\n",
    "            None,  # Globally unique ID for bookkeeping, unused in this case\n",
    "            text_a=x[DATA_COLUMN],\n",
    "            text_b=None,\n",
    "            label=x[LABEL_COLUMN]),\n",
    "        axis=1)\n",
    "\n",
    "    validation_InputExamples = test.apply(\n",
    "        lambda x: InputExample(\n",
    "            guid=\n",
    "            None,  # Globally unique ID for bookkeeping, unused in this case\n",
    "            text_a=x[DATA_COLUMN],\n",
    "            text_b=None,\n",
    "            label=x[LABEL_COLUMN]),\n",
    "        axis=1)\n",
    "\n",
    "    return train_InputExamples, validation_InputExamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_tf_dataset(examples, tokenizer, max_length=128):\n",
    "    features = []  # -> will hold InputFeatures to be converted later\n",
    "\n",
    "    for e in examples:\n",
    "        # Documentation is really strong for this method, so please take a look at it\n",
    "        input_dict = tokenizer.encode_plus(\n",
    "            e.text_a,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length,  # truncates if len(s) > max_length\n",
    "            return_token_type_ids=True,\n",
    "            return_attention_mask=True,\n",
    "            pad_to_max_length=True,\n",
    "            # padding=\"longest\", # pads to the right by default # CHECK THIS for pad_to_max_length\n",
    "            truncation=True)\n",
    "\n",
    "        input_ids, token_type_ids, attention_mask = (\n",
    "            input_dict[\"input_ids\"], input_dict[\"token_type_ids\"],\n",
    "            input_dict['attention_mask'])\n",
    "\n",
    "        features.append(\n",
    "            InputFeatures(input_ids=input_ids,\n",
    "                          attention_mask=attention_mask,\n",
    "                          token_type_ids=token_type_ids,\n",
    "                          label=e.label))\n",
    "\n",
    "    def gen():\n",
    "        for f in features:\n",
    "            yield (\n",
    "                {\n",
    "                    \"input_ids\": f.input_ids,\n",
    "                    \"attention_mask\": f.attention_mask,\n",
    "                    \"token_type_ids\": f.token_type_ids,\n",
    "                },\n",
    "                f.label,\n",
    "            )\n",
    "\n",
    "    return tf.data.Dataset.from_generator(\n",
    "        gen,\n",
    "        ({\n",
    "            \"input_ids\": tf.int32,\n",
    "            \"attention_mask\": tf.int32,\n",
    "            \"token_type_ids\": tf.int32\n",
    "        }, tf.int64),\n",
    "        (\n",
    "            {\n",
    "                \"input_ids\": tf.TensorShape([None]),\n",
    "                \"attention_mask\": tf.TensorShape([None]),\n",
    "                \"token_type_ids\": tf.TensorShape([None]),\n",
    "            },\n",
    "            tf.TensorShape([]),\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4366, 2) (480, 2)\n"
     ]
    }
   ],
   "source": [
    "# temp_test = pd.read_csv(\"../Crawled_data/labeled-daniil.csv\",\n",
    "#                         encoding=\"utf-8\",\n",
    "#                         sep=\";\")\n",
    "# # temp_test.head()\n",
    "\n",
    "# test = temp_test[[\"sentiment\", \"title\"]]\n",
    "# test = test.rename(columns={\"sentiment\": \"label\", \"title\": \"data\"})\n",
    "\n",
    "# test[\"label\"] = test[\"label\"].replace([\"positive\"], \"2\")\n",
    "# test[\"label\"] = test[\"label\"].replace([\"neutral\"], \"1\")\n",
    "# test[\"label\"] = test[\"label\"].replace([\"negative\"], \"0\")\n",
    "\n",
    "dataset = pd.read_csv(\"../dataset/all-data.csv\", encoding=\"ISO-8859-1\", engine=\"python\")\n",
    "\n",
    "dataset[\"label\"] = dataset[\"label\"].replace([\"positive\"], \"2\")\n",
    "dataset[\"label\"] = dataset[\"label\"].replace([\"neutral\"], \"1\")\n",
    "dataset[\"label\"] = dataset[\"label\"].replace([\"negative\"], \"0\")\n",
    "\n",
    "# for _, row in dataset.iterrows():\n",
    "#     if row[\"label\"] != \"0\" and row[\"label\"] != \"1\" and row[\"label\"] != \"2\":\n",
    "#         print(\"shit:\", row[\"label\"])\n",
    "\n",
    "test = dataset.sample(480)\n",
    "\n",
    "dataset = dataset.drop(test.index)\n",
    "\n",
    "print(dataset.shape, test.shape)\n",
    "\n",
    "# test.head()\n",
    "# dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2302: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "DATA_COLUMN = \"data\"\n",
    "LABEL_COLUMN = \"label\"\n",
    "\n",
    "train_InputExamples, validation_InputExamples = convert_data_to_examples(\n",
    "    dataset, test, DATA_COLUMN, LABEL_COLUMN)\n",
    "\n",
    "train_data = convert_examples_to_tf_dataset(list(train_InputExamples),\n",
    "                                            tokenizer)\n",
    "train_data = train_data.shuffle(100).batch(32).repeat(2)\n",
    "\n",
    "validation_data = convert_examples_to_tf_dataset(\n",
    "    list(validation_InputExamples), tokenizer)\n",
    "validation_data = validation_data.batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot convert 'passthrough' to EagerTensor of dtype float",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\barab\\OneDrive - České vysoké učení technické v Praze\\courses\\CE4045 - NLP\\assigment\\CZ4045-NLP\\BERT\\notebook.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/barab/OneDrive%20-%20%C4%8Cesk%C3%A9%20vysok%C3%A9%20u%C4%8Den%C3%AD%20technick%C3%A9%20v%20Praze/courses/CE4045%20-%20NLP/assigment/CZ4045-NLP/BERT/notebook.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m loss \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39msigmoid_cross_entropy_with_logits(logits\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mconstant([\u001b[39m1.\u001b[39m, \u001b[39m1.\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/barab/OneDrive%20-%20%C4%8Cesk%C3%A9%20vysok%C3%A9%20u%C4%8Den%C3%AD%20technick%C3%A9%20v%20Praze/courses/CE4045%20-%20NLP/assigment/CZ4045-NLP/BERT/notebook.ipynb#X11sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m                                                                    \u001b[39m1.\u001b[39m]),\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/barab/OneDrive%20-%20%C4%8Cesk%C3%A9%20vysok%C3%A9%20u%C4%8Den%C3%AD%20technick%C3%A9%20v%20Praze/courses/CE4045%20-%20NLP/assigment/CZ4045-NLP/BERT/notebook.ipynb#X11sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m                                                labels\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mconstant([\u001b[39m1.\u001b[39m, \u001b[39m1.\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/barab/OneDrive%20-%20%C4%8Cesk%C3%A9%20vysok%C3%A9%20u%C4%8Den%C3%AD%20technick%C3%A9%20v%20Praze/courses/CE4045%20-%20NLP/assigment/CZ4045-NLP/BERT/notebook.ipynb#X11sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m                                                                    \u001b[39m1.\u001b[39m]))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/barab/OneDrive%20-%20%C4%8Cesk%C3%A9%20vysok%C3%A9%20u%C4%8Den%C3%AD%20technick%C3%A9%20v%20Praze/courses/CE4045%20-%20NLP/assigment/CZ4045-NLP/BERT/notebook.ipynb#X11sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m metrics \u001b[39m=\u001b[39m [tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mmetrics\u001b[39m.\u001b[39mSparseCategoricalAccuracy(\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m)]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/barab/OneDrive%20-%20%C4%8Cesk%C3%A9%20vysok%C3%A9%20u%C4%8Den%C3%AD%20technick%C3%A9%20v%20Praze/courses/CE4045%20-%20NLP/assigment/CZ4045-NLP/BERT/notebook.ipynb#X11sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m model\u001b[39m.\u001b[39;49mcompile(optimizer\u001b[39m=\u001b[39;49moptimizer, loss\u001b[39m=\u001b[39;49mloss, metrics\u001b[39m=\u001b[39;49mmetrics)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/barab/OneDrive%20-%20%C4%8Cesk%C3%A9%20vysok%C3%A9%20u%C4%8Den%C3%AD%20technick%C3%A9%20v%20Praze/courses/CE4045%20-%20NLP/assigment/CZ4045-NLP/BERT/notebook.ipynb#X11sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m model\u001b[39m.\u001b[39mfit(train_data, epochs\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, validation_data\u001b[39m=\u001b[39mvalidation_data)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\transformers\\modeling_tf_utils.py:1297\u001b[0m, in \u001b[0;36mTFPreTrainedModel.compile\u001b[1;34m(self, optimizer, loss, metrics, loss_weights, weighted_metrics, run_eagerly, steps_per_execution, **kwargs)\u001b[0m\n\u001b[0;32m   1282\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompile\u001b[39m(\n\u001b[0;32m   1283\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   1284\u001b[0m     optimizer\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mrmsprop\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1291\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m   1292\u001b[0m ):\n\u001b[0;32m   1293\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1294\u001b[0m \u001b[39m    This is a thin wrapper that sets the model's loss output head as the loss if the user does not specify a loss\u001b[39;00m\n\u001b[0;32m   1295\u001b[0m \u001b[39m    function themselves.\u001b[39;00m\n\u001b[0;32m   1296\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1297\u001b[0m     \u001b[39mif\u001b[39;00m loss \u001b[39m==\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mpassthrough\u001b[39;49m\u001b[39m\"\u001b[39;49m:\n\u001b[0;32m   1298\u001b[0m         logger\u001b[39m.\u001b[39mwarning(\n\u001b[0;32m   1299\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mNo loss specified in compile() - the model\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms internal loss computation will be used as the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1300\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mloss. Don\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt panic - this is a common way to train TensorFlow models in Transformers! \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1301\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mTo disable this behaviour please pass a loss argument, or explicitly pass \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1302\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m`loss=None` if you do not want your model to compute a loss.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1303\u001b[0m         )\n\u001b[0;32m   1304\u001b[0m         loss \u001b[39m=\u001b[39m dummy_loss\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    100\u001b[0m     dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mas_dtype(dtype)\u001b[39m.\u001b[39mas_datatype_enum\n\u001b[0;32m    101\u001b[0m ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m--> 102\u001b[0m \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mEagerTensor(value, ctx\u001b[39m.\u001b[39;49mdevice_name, dtype)\n",
      "\u001b[1;31mTypeError\u001b[0m: Cannot convert 'passthrough' to EagerTensor of dtype float"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5,\n",
    "                                     epsilon=1e-08,\n",
    "                                     clipnorm=1.0)\n",
    "loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=tf.constant([1., 1.,\n",
    "                                                                   1.]),\n",
    "                                               labels=tf.constant([1., 1.,\n",
    "                                                                   1.]))\n",
    "metrics = [tf.keras.metrics.SparseCategoricalAccuracy('accuracy')]\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "model.fit(train_data, epochs=1, validation_data=validation_data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
