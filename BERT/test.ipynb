{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "from tensorflow.keras import preprocessing as kprocessing\n",
    "from tensorflow.keras import models, layers, optimizers\n",
    "\n",
    "import transformers\n",
    "\n",
    "# Visualization\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Text processing\n",
    "import re\n",
    "import nltk\n",
    "# import gensim.downloader as api\n",
    "\n",
    "# Various\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../dataset/all-data.csv\", encoding='latin-1')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming, Input -> X, Output -> y\n",
    "X_bert = df['data']\n",
    "y_bert_class = df['label']\n",
    "\n",
    "# Convert labels into a one-hot vector of size 5 (the number of distinct labels)\n",
    "lab = LabelBinarizer()\n",
    "lab.fit(y_bert_class)\n",
    "y_bert = lab.transform(y_bert_class)\n",
    "\n",
    "# Example (you can modify n)\n",
    "n = 100\n",
    "print('Coding of labels into a one-hot vector: ' + y_bert_class[n] + ' is ',\n",
    "      y_bert[n])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distil-bert tokenizer\n",
    "tokenizer_bert = transformers.DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "X_bert = [tokenizer_bert(text, padding='max_length', max_length = 512, truncation=True)['input_ids'] for text in X_bert]\n",
    "X_bert = np.array(X_bert, dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split\n",
    "X_bert_train, X_bert_test, y_bert_train, y_bert_test = train_test_split(X_bert, y_bert, test_size=0.3, random_state=42, stratify=df['label'])\n",
    "print('Shape of training data: ',X_bert_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = transformers.DistilBertConfig(dropout=0.2, attention_dropout=0.2)\n",
    "dbert = transformers.TFDistilBertModel.from_pretrained('distilbert-base-uncased', config=config, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a sampe of size 5 from the training data\n",
    "sample = X_bert_train[0:5]\n",
    "print('Object type: ', type(dbert(sample)))\n",
    "print('Output format (shape): ',dbert(sample)[0].shape)\n",
    "print('Output used as input for the classifier (shape): ', dbert(sample)[0][:,0,:].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_in = layers.Input(shape=(512,), name='input_token', dtype='int32')\n",
    "\n",
    "x = dbert(input_ids=input_ids_in)[0][:,0,:]\n",
    "x = layers.Dropout(0.2, name='dropout')(x)\n",
    "x = layers.Dense(64, activation='relu', name='dense')(x)\n",
    "x = layers.Dense(3, activation='softmax', name='classification')(x)\n",
    "\n",
    "dmodel = models.Model(inputs=input_ids_in, outputs = x)\n",
    "\n",
    "dmodel.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "dmodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "start_time = datetime.now()\n",
    "history = dmodel.fit(X_bert_train, y_bert_train, batch_size=32, shuffle=True, epochs=5, validation_data=(X_bert_test, y_bert_test))\n",
    "end_time = datetime.now()\n",
    "\n",
    "training_time_bert = (end_time - start_time).total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n",
    "ax[0].set(title='Loss')\n",
    "ax[0].plot(history.history['loss'], label='Training')\n",
    "ax[0].plot(history.history['val_loss'], label='Validation')\n",
    "ax[0].legend(loc=\"upper right\")\n",
    "\n",
    "ax[1].set(title='Accuracy')\n",
    "ax[1].plot(history.history['accuracy'], label='Training')\n",
    "ax[1].plot(history.history['val_accuracy'], label='Validation')\n",
    "ax[1].legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_bert = history.history['val_accuracy'][-1]\n",
    "print('Accuracy Training data: {:.1%}'.format(history.history['accuracy'][-1]))\n",
    "print('Accuracy Test data: {:.1%}'.format(history.history['val_accuracy'][-1]))\n",
    "print('Training time: {:.1f}s (or {:.1f} minutes)'.format(\n",
    "    training_time_bert, training_time_bert / 60))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
